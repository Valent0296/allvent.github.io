{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl2D7XoNyxA_",
        "outputId": "ca675a08-e7e0-41f1-f47d-c6da6cd4bc9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "mport pandas as pd\n",
        "import numpy as np\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, BatchEncoding, PreTrainedTokenizerBase, AutoModel\n",
        "from transformers import BertModel\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
        "import torch\n",
        "import math\n",
        "import os\n",
        "from typing import Any,  Union\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append(\"/MyDrive/dataset/\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "collapsed": true,
        "id": "jxoGQp7jzodM",
        "outputId": "522f5667-40ca-4b33-d5ba-cefc6ee3fc7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/dataset\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 text  label  len\n",
              "id                                                               \n",
              "0   No sirve para la calle  Tiene el gran problema...      1  614\n",
              "1   No me ha llegado  No me ha llegado  Estoy espe...      0   76\n",
              "2   Mal servicio   Producto no recibido  Aunque la...      0  160\n",
              "3   Mala experiencia  Muy disgustado con el produc...      0  138\n",
              "4   Luz bicicleta  Es muy buena luz y dura bastant...      4   59"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-82364545-c56e-46e1-bb42-fea80236352e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>len</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>No sirve para la calle  Tiene el gran problema...</td>\n",
              "      <td>1</td>\n",
              "      <td>614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>No me ha llegado  No me ha llegado  Estoy espe...</td>\n",
              "      <td>0</td>\n",
              "      <td>76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mal servicio   Producto no recibido  Aunque la...</td>\n",
              "      <td>0</td>\n",
              "      <td>160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mala experiencia  Muy disgustado con el produc...</td>\n",
              "      <td>0</td>\n",
              "      <td>138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Luz bicicleta  Es muy buena luz y dura bastant...</td>\n",
              "      <td>4</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-82364545-c56e-46e1-bb42-fea80236352e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-82364545-c56e-46e1-bb42-fea80236352e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-82364545-c56e-46e1-bb42-fea80236352e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b40fac88-ed84-4089-80a9-53700a1ed1fe\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b40fac88-ed84-4089-80a9-53700a1ed1fe')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b40fac88-ed84-4089-80a9-53700a1ed1fe button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#Mi sposto nella cartella per importare il file\n",
        "%cd /content/drive/MyDrive/dataset/\n",
        "\n",
        "#Importo il file tramite pandas e pulisco il contenuto text da caratteri di escape\n",
        "train = pd.read_csv(\"reviews_dataset.csv\", index_col='id')\n",
        "\n",
        "#Pulisco le recensioni nella colonna text da tutti i caratteri di escape come \\n\n",
        "train['text'] = train['text'].replace( r'\\\\[abfnrtv\\\\\"]', ' ', regex=True )\n",
        "#Pulisco le recensioni nella colonna text da tutti i caratteri non alfanumerici\n",
        "train['text'] = train['text'].replace( r'\\W', ' ', regex=True )\n",
        "\n",
        "\n",
        "train['len'] = train['text'].str.len()\n",
        "max_len = train['len'].max()\n",
        "\n",
        "n_star = 5\n",
        "\n",
        "dev_frac = 0.1\n",
        "test_frac = 0.1\n",
        "train_frac = (1 - dev_frac - test_frac)\n",
        "\n",
        "#Mischio il dataframe (estraggo in maniera casuale tutte le righe del DataFrame)\n",
        "both = train.sample(frac=1/20)\n",
        "\n",
        "#Creo i dataset per dev-train-set\n",
        "train_1 = both.sample(frac=train_frac)\n",
        "test = both.sample(frac=test_frac)\n",
        "dev = both.sample(frac=dev_frac)\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize = AutoTokenizer.from_pretrained(\"bert-base-cased\" )\n",
        "\n",
        "def transform_list_of_texts(\n",
        "    texts: list[str],\n",
        "    tokenizer: PreTrainedTokenizerBase,\n",
        "    chunk_size: int,\n",
        "    stride: int,\n",
        "    minimal_chunk_length: int\n",
        ") -> BatchEncoding:\n",
        "    model_inputs = [\n",
        "        transform_single_text(text, tokenizer, chunk_size, stride, minimal_chunk_length)\n",
        "        for text in texts\n",
        "    ]\n",
        "    input_ids = [model_input[0] for model_input in model_inputs]\n",
        "    attention_mask = [model_input[1] for model_input in model_inputs]\n",
        "    tokens = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
        "    return BatchEncoding(tokens)\n",
        "\n",
        "\n",
        "def transform_single_text(\n",
        "    text: str,\n",
        "    tokenizer: PreTrainedTokenizerBase,\n",
        "    chunk_size: int,\n",
        "    stride: int,\n",
        "    minimal_chunk_length: int,\n",
        ") -> tuple[Tensor, Tensor]:\n",
        "    \"\"\"Transforms (the entire) text to model input of BERT model.\"\"\"\n",
        "    tokens = tokenize_whole_text(text, tokenizer)\n",
        "    input_id_chunks, mask_chunks = split_tokens_into_smaller_chunks(tokens, chunk_size, stride, minimal_chunk_length)\n",
        "    add_special_tokens_at_beginning_and_end(input_id_chunks, mask_chunks)\n",
        "    add_padding_tokens(input_id_chunks, mask_chunks)\n",
        "    input_ids, attention_mask = stack_tokens_from_all_chunks(input_id_chunks, mask_chunks)\n",
        "    return input_ids, attention_mask\n",
        "\n",
        "\n",
        "def tokenize_whole_text(text: str, tokenizer: PreTrainedTokenizerBase) -> BatchEncoding:\n",
        "    \"\"\"Tokenizes the entire text without truncation and without special tokens.\"\"\"\n",
        "    tokens = tokenizer(text, add_special_tokens=False, truncation=False, return_tensors=\"pt\")\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def tokenize_text_with_truncation(\n",
        "    text: str, tokenizer: PreTrainedTokenizerBase, maximal_text_length: int\n",
        ") -> BatchEncoding:\n",
        "    \"\"\"Tokenizes the text with truncation to maximal_text_length and without special tokens.\"\"\"\n",
        "    tokens = tokenizer(\n",
        "        text, add_special_tokens=False, max_length=maximal_text_length, truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def split_tokens_into_smaller_chunks(\n",
        "    tokens: BatchEncoding,\n",
        "    chunk_size: int,\n",
        "    stride: int,\n",
        "    minimal_chunk_length: int,\n",
        ") -> tuple[list[Tensor], list[Tensor]]:\n",
        "    \"\"\"Splits tokens into overlapping chunks with given size and stride.\"\"\"\n",
        "    input_id_chunks = split_overlapping(tokens[\"input_ids\"][0], chunk_size, stride, minimal_chunk_length)\n",
        "    mask_chunks = split_overlapping(tokens[\"attention_mask\"][0], chunk_size, stride, minimal_chunk_length)\n",
        "    return input_id_chunks, mask_chunks\n",
        "\n",
        "\n",
        "def add_special_tokens_at_beginning_and_end(input_id_chunks: list[Tensor], mask_chunks: list[Tensor]) -> None:\n",
        "    \"\"\"\n",
        "    Adds special CLS token (token id = 101) at the beginning.\n",
        "    Adds SEP token (token id = 102) at the end of each chunk.\n",
        "    Adds corresponding attention masks equal to 1 (attention mask is boolean).\n",
        "    \"\"\"\n",
        "    if len(input_id_chunks) == 0:\n",
        "        input_id_chunks.append(torch.Tensor([101, 102]))\n",
        "        mask_chunks.append(torch.Tensor([1, 1]))\n",
        "        return\n",
        "    for i in range(len(input_id_chunks)):\n",
        "        # adding CLS (token id 101) and SEP (token id 102) tokens\n",
        "        input_id_chunks[i] = torch.cat([Tensor([101]), input_id_chunks[i], Tensor([102])])\n",
        "        # adding attention masks  corresponding to special tokens\n",
        "        mask_chunks[i] = torch.cat([Tensor([1]), mask_chunks[i], Tensor([1])])\n",
        "\n",
        "\n",
        "def add_padding_tokens(input_id_chunks: list[Tensor], mask_chunks: list[Tensor]) -> None:\n",
        "    \"\"\"Adds padding tokens (token id = 0) at the end to make sure that all chunks have exactly 512 tokens.\"\"\"\n",
        "    for i in range(len(input_id_chunks)):\n",
        "        # get required padding length\n",
        "        pad_len = 512 - input_id_chunks[i].shape[0]\n",
        "        # check if tensor length satisfies required chunk size\n",
        "        if pad_len > 0:\n",
        "            # if padding length is more than 0, we must add padding\n",
        "            input_id_chunks[i] = torch.cat([input_id_chunks[i], Tensor([0] * pad_len)])\n",
        "            mask_chunks[i] = torch.cat([mask_chunks[i], Tensor([0] * pad_len)])\n",
        "\n",
        "\n",
        "def stack_tokens_from_all_chunks(input_id_chunks: list[Tensor], mask_chunks: list[Tensor]) -> tuple[Tensor, Tensor]:\n",
        "    \"\"\"Reshapes data to a form compatible with BERT model input.\"\"\"\n",
        "    input_ids = torch.stack(input_id_chunks)\n",
        "    attention_mask = torch.stack(mask_chunks)\n",
        "\n",
        "    return input_ids.long(), attention_mask.int()\n",
        "\n",
        "\n",
        "def split_overlapping(tensor: Tensor, chunk_size: int, stride: int, minimal_chunk_length: int) -> list[Tensor]:\n",
        "    \"\"\"Helper function for dividing 1-dimensional tensors into overlapping chunks.\"\"\"\n",
        "    result = [tensor[i : i + chunk_size] for i in range(0, len(tensor), stride)]\n",
        "    if len(result) > 1:\n",
        "        # ignore chunks with less than minimal_length number of tokens\n",
        "        result = [x for x in result if len(x) >= minimal_chunk_length]\n",
        "    return result\n",
        "\n"
      ],
      "metadata": {
        "id": "inZyPKvLJ0aI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tok = transform_list_of_texts(train_1['text'].to_list(), tokenize, 510, 510, 510 )\n",
        "test_tok = transform_list_of_texts(test['text'].to_list(), tokenize, 510, 510, 510 )\n",
        "\n",
        "class TokenizedDataset(Dataset):\n",
        "    \"\"\"Dataset for tokens with optional labels.\"\"\"\n",
        "\n",
        "    def __init__(self, tokens: BatchEncoding, labels: list):\n",
        "        self.input_ids = tokens[\"input_ids\"]\n",
        "        self.attention_mask = [att > 0  for att in tokens[\"attention_mask\"]]\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Union[tuple[Tensor, Tensor, Any], tuple[Tensor, Tensor]]:\n",
        "        if self.labels:\n",
        "            return self.input_ids[idx], self.attention_mask[idx], self.labels[idx]\n",
        "        return self.input_ids[idx], self.attention_mask[idx]\n",
        "\n",
        "train_dataset = TokenizedDataset(train_tok, train_1['label'].to_list())\n",
        "test_dataset = TokenizedDataset(test_tok, test['label'].to_list())\n"
      ],
      "metadata": {
        "id": "lLUsTcoHLO6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87a7fc71-65d8-49f3-afdc-dca5e48e10c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = tokenize.vocab_size\n",
        "print(vocab)"
      ],
      "metadata": {
        "id": "RcawOfSDsuEj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84519dcc-837d-40a6-a1b3-ca29a77fd889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "QzAMDH97j8FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlboBdSf1A71"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, accumulation_steps):\n",
        "\n",
        "  model.train()\n",
        "  size = 0\n",
        "  correct = 0\n",
        "  for step, batch in enumerate(dataloader):\n",
        "    input_tokens, label = batch[0].to(device), batch[-1]\n",
        "    attention_mask = batch[1].to(device)\n",
        "    number_of_chunks = [len(x) for x in input_tokens]\n",
        "    one_hot_label = nn.functional.one_hot(label, num_classes=n_star).to(float).to(device)\n",
        "    # concateno tutti i token di input in uno singolo\n",
        "    input_tokens_combined = []\n",
        "    for x in input_tokens:\n",
        "        input_tokens_combined.extend(x.tolist())\n",
        "\n",
        "    input_tokens_combined_tensors = torch.stack([torch.tensor(x).to(device) for x in input_tokens_combined])\n",
        "\n",
        "    # concateno tutte le maschere di Padding per il meccanismo dell'attenzione\n",
        "    attention_mask_combined = []\n",
        "    for x in attention_mask:\n",
        "        attention_mask_combined.extend(x.tolist())\n",
        "\n",
        "    attention_mask_combined_tensors = torch.stack(\n",
        "        [torch.tensor(x).to(device) for x in attention_mask_combined]\n",
        "    )\n",
        "\n",
        "    # ottengo le probabilità dall'encoder del transformer\n",
        "    logits = model(input_tokens_combined_tensors, attention_mask_combined_tensors)\n",
        "\n",
        "    # divido in pezzi i rrisultati per calcolarne infine la media finale\n",
        "    logits_split = logits.split(number_of_chunks, dim=0)\n",
        "\n",
        "    pooled_logits = torch.stack([torch.mean(x, dim=0) for x in logits_split])\n",
        "    probs = nn.Softmax(1)(pooled_logits)\n",
        "    #Calcolo gli one hot label dalle etichette per il calcolo della funzione di loss\n",
        "    loss = loss_fn(probs, one_hot_label)\n",
        "\n",
        "    #Backpropagation\n",
        "    loss.backward()\n",
        "    #Caolcolo l'accuratezza del singolo batch\n",
        "    out = probs.argmax(1).to(\"cpu\")\n",
        "    correct += ( out == label).to(float).sum(0).item()\n",
        "    size += 64\n",
        "    curr_acc = correct/size\n",
        "    #optimizer.step()\n",
        "    if ((step + 1) % accumulation_steps == 0) or (step + 1 == len(dataloader)):\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    if step % 100 == 0:\n",
        "      loss= loss.item()\n",
        "\n",
        "      print(f\"Funzione di loss: {loss:>7f} Errore train: \\n Accuratezza: {(100*curr_acc):>0.1f}%, step[{step}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = 0\n",
        "    correct = 0\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for step, batch in enumerate(dataloader):\n",
        "        input_tokens, label = batch[0].to(device), batch[-1]\n",
        "        attention_mask = batch[1].to(device)\n",
        "        number_of_chunks = [len(x) for x in input_tokens]\n",
        "        one_hot_label = nn.functional.one_hot(label, num_classes=n_star).to(float).to(device)\n",
        "        # concateno tutti i token di input in uno singolo\n",
        "        input_tokens_combined = []\n",
        "        for x in input_tokens:\n",
        "            input_tokens_combined.extend(x.tolist())\n",
        "\n",
        "        input_tokens_combined_tensors = torch.stack([torch.tensor(x).to(device) for x in input_tokens_combined])\n",
        "\n",
        "        # concateno tutte le maschere di Padding per il meccanismo dell'attenzione\n",
        "        attention_mask_combined = []\n",
        "        for x in attention_mask:\n",
        "            attention_mask_combined.extend(x.tolist())\n",
        "\n",
        "        attention_mask_combined_tensors = torch.stack(\n",
        "            [torch.tensor(x).to(device) for x in attention_mask_combined]\n",
        "        )\n",
        "\n",
        "        # ottengo le probabilità dall'encoder del transformer\n",
        "        logits = model(input_tokens_combined_tensors, attention_mask_combined_tensors)\n",
        "\n",
        "        # divido in pezzi i rrisultati per calcolarne infine la media finale\n",
        "        logits_split = logits.split(number_of_chunks, dim=0)\n",
        "\n",
        "        pooled_logits = torch.stack([torch.mean(x, dim=0) for x in logits_split])\n",
        "        probs = nn.Softmax(1)(pooled_logits)\n",
        "        #Calcolo gli one hot label dalle etichette per il calcolo della funzione di loss\n",
        "        loss = loss_fn(probs, one_hot_label)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        #Caolcolo l'accuratezza del singolo batch\n",
        "        out = probs.argmax(1).to(\"cpu\")\n",
        "        correct += ( out == label).to(float).sum(0).item()\n",
        "    size = len(test['text'])\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Errore test: \\n Accuratezza: {(100*correct):>0.1f}%, Media loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_iQU_iyGqaP"
      },
      "outputs": [],
      "source": [
        "class BertNN(nn.Module):\n",
        "    def __init__(self, model: BertModel, num_labels: int):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "        # classification head\n",
        "        self.fin_seq = nn.Sequential(\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128,128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Linear(64,num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids: Tensor, attention_mask: Tensor) -> Tensor:\n",
        "        x = self.model(input_ids, attention_mask)\n",
        "        x = x[0][:, 0, :]  # take <s> token (equiv. to [CLS])\n",
        "\n",
        "        # classification head\n",
        "        x = self.fin_seq(x)\n",
        "        return x\n",
        "\n",
        "bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertNN(model=bert, num_labels=5)\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if \"encoder.layer.9\" in name or \"encoder.layer.10\" in name or \"encoder.layer.11\" in name or \"fin_seq\" in name:\n",
        "        param.requires_grad = True  # Sblocca solo gli ultimi due layer\n",
        "    else:\n",
        "        param.requires_grad = False  # Congela gli altri\n",
        "\n",
        "for _, param in model.fin_seq.named_parameters():\n",
        "    if param.data.dim() == 2:\n",
        "        nn.init.xavier_uniform_(param)\n",
        "\n",
        "\n",
        "if torch.cuda.is_available() and torch.cuda.device_count():\n",
        "  model = nn.DataParallel(model)\n",
        "\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum = 0.7)\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {\"params\": model.module.model.parameters(), \"lr\": 5e-6},  # Learning rate basso per BERT\n",
        "    {\"params\": model.module.fin_seq.parameters(), \"lr\": 1e-1}    # Learning rate più alto per FC\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY8lA1L6H4rU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7903581-c5b8-4e5f-990b-4dfa4545fcb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0\n",
            "Funzione di loss: 1.653052 Errore train: \n",
            " Accuratezza: 23.4%, step[0]\n",
            "Funzione di loss: 1.623232 Errore train: \n",
            " Accuratezza: 20.6%, step[100]\n",
            "Funzione di loss: 1.627660 Errore train: \n",
            " Accuratezza: 20.8%, step[200]\n",
            "Epoch:  1\n",
            "Funzione di loss: 1.618555 Errore train: \n",
            " Accuratezza: 20.3%, step[0]\n",
            "Funzione di loss: 1.596468 Errore train: \n",
            " Accuratezza: 22.0%, step[100]\n",
            "Funzione di loss: 1.614030 Errore train: \n",
            " Accuratezza: 21.9%, step[200]\n",
            "Epoch:  2\n",
            "Funzione di loss: 1.604040 Errore train: \n",
            " Accuratezza: 21.9%, step[0]\n",
            "Funzione di loss: 1.590026 Errore train: \n",
            " Accuratezza: 22.6%, step[100]\n",
            "Funzione di loss: 1.571468 Errore train: \n",
            " Accuratezza: 22.8%, step[200]\n",
            "Epoch:  3\n",
            "Funzione di loss: 1.635813 Errore train: \n",
            " Accuratezza: 17.2%, step[0]\n",
            "Funzione di loss: 1.588374 Errore train: \n",
            " Accuratezza: 24.1%, step[100]\n",
            "Funzione di loss: 1.592777 Errore train: \n",
            " Accuratezza: 25.1%, step[200]\n",
            "Errore test: \n",
            " Accuratezza: 26.9%, Media loss: 1.577882 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(4):\n",
        "  print(\"Epoch: \", epoch)\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer, 10)\n",
        "\n",
        "\n",
        "test_loop(test_dataloader, model, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ba_0KzNk-Fa"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/dataset/\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}